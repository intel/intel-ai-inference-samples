{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4724019",
   "metadata": {},
   "source": [
    "Copyright (C) 2024 Intel Corporation\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Sagemaker inference with Intel optimizations\n",
    "\n",
    "## Agenda\n",
    "0. Prerequisites\n",
    "1. Build Deep Learning Container and push it to AWS ECR\n",
    "2. Create a Torchserve file and put it on S3 bucket\n",
    "3. Create AWS Sagemaker endpoint\n",
    "4. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a82916",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Install all libraries required to run the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ae516",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.175.0\" --upgrade --quiet\n",
    "!pip install awscli boto3 botocore numpy s3transfer torch-model-archiver==0.8.1 torchserve==0.8.2 --upgrade --quiet\n",
    "!pip install huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1ae90",
   "metadata": {},
   "source": [
    "Remember also that you have all required accesses on you AWS role. To run this example you're going to need following accesses:\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "- AmazonS3FullAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdbd705",
   "metadata": {},
   "source": [
    "**Define also following variables.** These variables are needed for the Deep Learning containers to build the Docker and push it to the AWS ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "current_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID = \"\"\n",
    "REPOSITORY_NAME = \"pytorch_inference\"\n",
    "REGION = \"\"\n",
    "# modify this based on your S3 Bucket name\n",
    "S3_BUCKET_NAME = \"\" # s3://<s3 bucket name>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these variable names based on S3 Bucket name and ECR url\n",
    "import os\n",
    "tag = f\"2.2.0-cpu-intel-py310-ubuntu20.04-sagemaker-llm-{current_datetime}\"\n",
    "ECR_URL = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPOSITORY_NAME}:{tag}\"\n",
    "S3_URL = os.path.join(S3_BUCKET_NAME, \"llm.tar.gz\")\n",
    "endpoint_name = \"llm-ipex\"\n",
    "ECR_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a092c8",
   "metadata": {},
   "source": [
    "### Build a docker container and push it to AWS ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952893b3",
   "metadata": {},
   "source": [
    "If you don't have Docker image prepared beforehand, build the image with all required intel optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review Docker\n",
    "!cat docker/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build docker image\n",
    "!docker build -t $ECR_URL docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35807e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to ECR\n",
    "!aws ecr get-login-password --region {REGION} | docker login --username AWS --password-stdin {ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push docker image\n",
    "!docker push $ECR_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66223ab",
   "metadata": {},
   "source": [
    "### Create a Torchserve file and put it on S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214eb033",
   "metadata": {},
   "source": [
    "All information about a model are stored in `model/model-config.yaml`. Please, put the model you'd like to run in model_name param. The endpoint has been tested on `Salesforce/codegen25-7b-multi`. Here's how to create a torchserve file and put it on S3 bucket required to run the endpoint with the container.\n",
    "\n",
    "In order to change batch size, max length or max new tokens of the model, modify fields in model-config.yaml before creating the Torchserve file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae63f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd model && cat model-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e5a88d",
   "metadata": {},
   "source": [
    "To generate a Torchserve file use following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"model/model-config.yaml\") as stream:\n",
    "    model_name = yaml.safe_load(stream)[\"handler\"][\"model_name\"]\n",
    "\n",
    "if model_name == \"\":\n",
    "    raise Exception((\"Specify model_name in model/model-config.yaml\"))\n",
    "\n",
    "# Create torchserve model archive\n",
    "!cd model && torch-model-archiver --force --model-name llm --version 1.0 --handler llm_handler.py --config-file model-config.yaml --archive-format tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51df9e",
   "metadata": {},
   "source": [
    "Next, copy the model into an S3 bucket of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd model && aws s3 cp llm.tar.gz $S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596150e",
   "metadata": {},
   "source": [
    "### Create AWS Sagemaker endpoint\n",
    "\n",
    "Next step is to deploy the model to AWS Sagemaker and create an endpoint in order to run inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto3_session = boto3.session.Session(region_name=REGION)\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)\n",
    "region = sess._region_name\n",
    "account = sess.account_id()\n",
    "\n",
    "bucket_name = sess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "print(f'account={account}, region={region}, role={role}, output_path={output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d557191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Model\n",
    "\n",
    "instance_type = \"ml.m7i.8xlarge\"\n",
    "sagemaker_name = sagemaker.utils.name_from_base(endpoint_name)\n",
    "\n",
    "model = Model(\n",
    "    name=\"torchserve-llm-ipex\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    # Enable SageMaker uncompressed model artifacts\n",
    "    model_data=S3_URL,\n",
    "    image_uri=ECR_URL,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    env={\"TS_INSTALL_PY_DEP_PER_MODEL\": \"true\",\n",
    "         \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"0\",\n",
    "         \"SAGEMAKER_REGION\": region},\n",
    ")\n",
    "print(sagemaker_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3776da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=sagemaker_name,\n",
    "    #volume_size=32, # increase the size to store large model\n",
    "    model_data_download_timeout=3600, # increase the timeout to download large model\n",
    "    container_startup_health_check_timeout=600, # increase the timeout to load large model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa9274",
   "metadata": {},
   "source": [
    "You can inspect the logs to check whether the model has been deployed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb84e6",
   "metadata": {},
   "source": [
    "### Invoke the endpoint\n",
    "\n",
    "Once the model is deployed, invoke the sample response with following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1132b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "task = \"Write a python function to compute the factorial of an integer.\"\n",
    "\n",
    "custom_attributes = \"c000b4f9-df62-4c85-a0bf-7c525f9104a4\"  # An example of a trace ID.\n",
    "content_type = \"text/plain\"                           # The MIME type of the input data in the request body.\n",
    "accept = \"*/*\"                                              # The desired MIME type of the inference in the response.\n",
    "\n",
    "import io\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        data = self.buff.getvalue()\n",
    "        \n",
    "    def scan_lines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0\n",
    "\n",
    "start_time = time.time()\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=sagemaker_name, \n",
    "    CustomAttributes=custom_attributes, \n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=task)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "parser = Parser()\n",
    "for event in response['Body']:\n",
    "    parser.write(event['PayloadPart']['Bytes'])\n",
    "    for line in parser.scan_lines():\n",
    "        print(\"\\n\", line.decode(\"utf-8\"), end=' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc9b67",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "Once you will be done running the endpoint, you can delete it by using following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=sagemaker_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d3983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
