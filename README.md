# Intel® AI Inference Samples

## Description
Intel® AI Inference Samples provide example code for deploying optimized inference in Intel platforms. 

## Samples List
- [Serving optimized BERT models with Intel® Extension for PyTorch (IPEX), Intel® Distribution of OpenVINO™ toolkit and Triton Inference Server framework](./serving_with_ipex_openvino_triton)
- [Optimized Large Language Model (LLM) GPT-j with Intel® Extension for PyTorch (IPEX)](./gpt-j_with_ipex/)
- [Serving optimized DenseNet model with Intel® Extension for PyTorch (IPEX) and Triton Inference Server framework](./serving_densnet_with_ipex_triton)
- [Deploy optimized Codegen model with Intel® Extension for PyTorch (IPEX) and Torchserve on AWS Sagemaker](./deploy_codegen_with_ipex_torchserve_sagemaker)
## Support
Please submit your questions, feature requests, and bug reports on the [GitHub issues page](https://github.com/intel/intel-ai-inference-samples/issues).

## License 
Intel® AI Inference Samples is licensed under Apache License Version 2.0. Refer to the [LICENSE](./LICENSE) file for the full license text and copyright notice.

This distribution includes third party software governed by separate license terms.

This third party software, even if included with the distribution of the Intel software, may be governed by separate license terms, including without limitation, third party license terms, other Intel software license terms, and open source software license terms. These separate license terms govern your use of the third party programs as set forth in the [THIRD-PARTY-PROGRAMS](./serving_with_ipex_openvino_triton/THIRD-PARTY-PROGRAMS) file.

## Trademark Information
Intel, the Intel logo, OpenVINO, the OpenVINO logo and Intel Xeon are trademarks of Intel Corporation or its subsidiaries.
* Other names and brands may be claimed as the property of others.

&copy;Intel Corporation
